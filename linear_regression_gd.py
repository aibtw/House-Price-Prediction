import sys
import numpy as np
from numpy import random as rnd
from matplotlib import pyplot as plt


def train():
    # ---------------------- Read input ---------------------- #
    x = np.transpose(np.loadtxt("train.csv", delimiter=',', skiprows=1, usecols=(1, 4, 17, 18, 19, 43, 44, 46, 62, 77)))
    y = np.loadtxt("train.csv", delimiter=',', skiprows=1, usecols=-1)

    (x_m, x_n) = x.shape  # The dimensions of x are [m rows by n columns]
    num_of_features = x_m

    # ---------------------- Feature Scaling ---------------------- #
    s_factor = np.amax(x, axis=1)
    x = x / s_factor[:, None]

    # --------- Theta random start point.   Thetas = [Th0, Th1, Th2 ...] --------- #
    thetas = rnd.random(num_of_features + 1)
    # ---------------------- Learning rate ---------------------- #
    lr = 0.005
    # ---------------------- Initialize loss ---------------------- #
    loss = list()
    # ---------------------- Manipulate input data for processing ---------------------- #
    """ trainX = [1 , 1,  1  ... ] <--- To be multiplied by Theta0
                 [x1, x2, x3 ... ] <--- Feature 1, multiplies by Theta1
                 [v1, v2, v3 ... ] <--- Feature 2, multiplied by Theta2
                 ... etc
    """
    trainX = np.vstack([np.ones(x_n), x])
    """ trainY = [Y1 , Y2,  Y3  ... ] """
    trainY = y

    # # ==================== Main Linear loop ==================== #
    # for i in range(10000):
    #     """ hypothesis hyp = [h1 , h2,  h3  ... ]. (This is the guess generated by current value of theta)"""
    #     hyp = np.dot(thetas, trainX)
    #
    #     """Calculate loss"""
    #     loss.append(np.sum((hyp - trainY) ** 2) / x_n)
    #
    #     """Update function = trainX * loss function"""
    #     Grad = np.dot(trainX, np.vstack(hyp - trainY) / x_n)
    #     """Theoretically Correct"""
    #
    #     """Update theta"""
    #     thetas = thetas - (lr * np.hstack(Grad))
    #     """Theoretically correct"""
    #
    #     relative_loss = abs((loss[i] - loss[i - 1]) / loss[i] * 100)
    #     if relative_loss <= 0.01 and i != 0:
    #         break
    #     plt.plot(loss)
    #     # plt.pause(0.0001)
    #
    # # Output Area
    # print(thetas)
    # np.save('linear_thetas', thetas)  # Save thetas for the test phase.
    # with open('Linear_Train_Results.txt', 'w') as wr:
    #     wr.write("The loss value for train model is: \n" + str(loss[-1]))
    #     wr.write("\nThe Theta values for training model are: \n")
    #     wr.write(np.array2string(thetas))
    # plt.show()

    # ======================================= Degree 2 =================================================== #

    trainX = np.vstack([trainX, np.square(x)])
    (x_m, x_n) = trainX.shape
    num_of_features = x_m
    thetas = rnd.random(num_of_features)
    loss = list()
    # # ==================== Main Degree 2 loop ==================== #
    # for i in range(10000):
    #     """ hypothesis hyp = [h1 , h2,  h3  ... ]. (This is the guess generated by current value of theta)"""
    #     hyp = np.dot(thetas, trainX)
    #
    #     """Calculate loss"""
    #     loss.append(np.sum((hyp - trainY) ** 2) / x_n)
    #     """Update function = trainX * loss function"""
    #     Grad = np.dot(trainX, np.vstack(hyp - trainY) / x_n)
    #     """Theoretically Correct"""
    #
    #     """Update theta"""
    #     thetas = thetas - (lr * np.hstack(Grad))
    #     """Theoretically correct"""
    #
    #     relative_loss = abs((loss[i] - loss[i - 1]) / loss[i] * 100)
    #     if relative_loss <= 0.01 and i != 0:
    #         print(i)
    #         print(hyp)
    #         break
    #     plt.plot(loss)
    #     # plt.pause(0.0001)
    #
    # # Output Area
    # print(thetas)
    # np.save('Polynomial_Deg2_thetas', thetas)  # Save thetas for the test phase.
    # with open('Polynomial_Deg2_Train_Results.txt', 'w') as wr:
    #     wr.write("The loss value for train model is: \n" + str(loss[-1]))
    #     wr.write("\nThe Theta values for training model are: \n")
    #     wr.write(np.array2string(thetas))
    # plt.show()

    # ======================================= Degree 3 =================================================== #
    trainX = np.vstack([trainX, np.power(x, 3)])
    (x_m, x_n) = trainX.shape
    num_of_features = x_m
    thetas = rnd.random(num_of_features)
    loss = list()
    # ==================== Main Degree 2 loop ==================== #
    for i in range(15000):
        """ hypothesis hyp = [h1 , h2,  h3  ... ]. (This is the guess generated by current value of theta)"""
        hyp = np.dot(thetas, trainX)

        """Calculate loss"""
        loss.append(np.sum((hyp - trainY) ** 2) / x_n)
        """Update function = trainX * loss function"""
        Grad = np.dot(trainX, np.vstack(hyp - trainY) / x_n)
        """Theoretically Correct"""

        """Update theta"""
        thetas = thetas - (lr * np.hstack(Grad))
        """Theoretically correct"""

        relative_loss = abs((loss[i] - loss[i - 1]) / loss[i] * 100)
        if relative_loss <= 0.01 and i != 0:
            print(i)
            print(hyp)
            break
        plt.plot(loss)
        # plt.pause(0.0001)

    # Output Area
    print(thetas)
    np.save('Polynomial_Deg3_thetas', thetas)  # Save thetas for the test phase.
    with open('Polynomial_Deg3_Train_Results.txt', 'w') as wr:
        wr.write("The loss value for train model is: \n" + str(loss[-1]))
        wr.write("\nThe Theta values for training model are: \n")
        wr.write(np.array2string(thetas))
    plt.show()


# ===================================================================================================== #
# ===========================================  Testing  =============================================== #
# ===================================================================================================== #
def test():
    # ---------------------- Reading General input ---------------------- #
    x = np.transpose(np.genfromtxt("test.csv", delimiter=',', skip_header=1,
                                   usecols=(1, 4, 17, 18, 19, 43, 44, 46, 62, 77),  # use only the specified columns
                                   filling_values=0))
    y = np.loadtxt("test.csv", delimiter=',', skiprows=1, usecols=-1)
    # ---------------------- Feature Scaling ---------------------- #
    s_factor = np.amax(x, axis=1)
    x = x / s_factor[:, None]
    (x_m, x_n) = x.shape

    print("\n\n#==================== X and Y ====================#")
    print(x)
    print(y)

    # ====================== Test Linear ======================== #
    print("\n\n#==================== Linear Testing ====================#")
    train_thetas = np.load("linear_thetas.npy")

    testX = np.vstack([np.ones(x_n), x])
    testY = y
    hyp = np.dot(train_thetas, testX)
    print("Hypothesis Overview:")
    print(hyp)
    test_loss = np.sum((hyp - testY) ** 2) / x_n
    test_acc = np.sum(abs(hyp - testY)) / x_n
    print("Linear Test Loss is: " + str(test_loss))
    print("Linear Test accuracy is: " + str(test_acc))

    # ====================== Test Linear Done ======================== #

    # ==================== Test 2nd degree ======================== #
    print("\n\n#==================== 2nd degree Testing ====================#")
    # ---------------------- Reading input ---------------------- #
    train_thetas = np.load("Polynomial_Deg2_thetas.npy")
    # ---------------------- update variables ---------------------- #
    testX = np.vstack([testX, np.square(x)])
    hyp = np.dot(train_thetas, testX)
    print("Hypothesis Overview:")
    print(hyp)
    test_loss = np.sum((hyp - testY) ** 2) / x_n
    test_acc = np.sum(abs(hyp - testY)) / x_n
    print("Linear Test Loss is: " + str(test_loss))
    print("Linear Test accuracy is: " + str(test_acc))
    # ==================== Test 2nd degree done ======================== #

    # ==================== Test 3rd degree ======================== #
    print("\n\n#==================== 3rd degree Testing ====================#")
    # ---------------------- Reading input ---------------------- #
    train_thetas = np.load("Polynomial_Deg3_thetas.npy")
    # ---------------------- update variables ---------------------- #
    testX = np.vstack([testX, np.power(x, 3)])
    hyp = np.dot(train_thetas, testX)
    print("Hypothesis Overview:")
    print(hyp)
    test_loss = np.sum((hyp - testY) ** 2) / x_n
    test_acc = np.sum(abs(hyp - testY)) / x_n
    print("Linear Test Loss is: " + str(test_loss))
    print("Linear Test accuracy is: " + str(test_acc))
    # ==================== Test 3rd degree done ======================== #


if __name__ == "__main__":
    if sys.argv[1] == "T":
        train()
    elif sys.argv[1] == "V":
        test()
    else:
        print("wrong command line argument")
        exit()
